{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Importing the libraries**"
      ],
      "metadata": {
        "id": "FTvYMZ5Ny1yO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhZuk3g4ECTn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "pc = Pinecone(api_key=\"84df92fd-e815-4e78-8432-10c84d302891\")\n",
        "from groq import Groq\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# !python -m spacy download en_core_web_sm\n"
      ],
      "metadata": {
        "id": "B5L9YBlAwrzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Setting up\n",
        "## 1. API keys (Groq & Pinecone) P.S - Since Open-AI API was a paid version, so I used Groq API with mixtral-8 7b llm which is free with very fast inference speed.\n",
        "## 2.Initialising pinecone, groq, embeddings and spacy.\n",
        "* I have used all-MiniLM-L6-v2 embedding model because its lightweight and yet very impressive performance. Storage and retrival with pinecone becomes compatible.\n",
        "\n",
        "* I have used spacy for paragraph segmentation in splitting the text. I have tried to implement ynamic Chunk Sizing with Semantic Segmentation."
      ],
      "metadata": {
        "id": "Fw5lOQjtzMvH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Seting up API keys\n",
        "os.environ['PINECONE_API_KEY'] = '4f4ce604-3786-40bf-a0b9-d9b439bd23f1'\n",
        "os.environ['GROQ_API_KEY'] = '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'\n",
        "\n",
        "# initialising pinecone\n",
        "pc = Pinecone(api_key=os.environ['PINECONE_API_KEY'])\n",
        "\n",
        "# setting groq client\n",
        "groq_client = Groq(api_key=os.environ['GROQ_API_KEY'])\n",
        "\n",
        "# initialise embeddings\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glW3OBxLEgT4",
        "outputId": "9330d7c7-6e40-42b1-8441-8b8014af17ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing semantic chunking\n",
        "* Instead of splitting the text into fixed-size chunks, I tried to implement a dynamic chunking strategy based on semantic coherence. This approach would ensure that each chunk contains complete and related information, potentially improving the relevance of retrieved context."
      ],
      "metadata": {
        "id": "vWCeIdT81xW8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ukeVtHFR1Z5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def semantic_chunking(text, min_chunk_size=100, max_chunk_size=500):\n",
        "    doc = nlp(text)\n",
        "    sentences = list(doc.sents)\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    current_chunk_size = 0\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentence_text = sentence.text.strip()\n",
        "        sentence_length = len(sentence_text)\n",
        "\n",
        "        if current_chunk_size + sentence_length > max_chunk_size and current_chunk:\n",
        "            chunks.append(\" \".join(current_chunk))\n",
        "            current_chunk = []\n",
        "            current_chunk_size = 0\n",
        "\n",
        "        current_chunk.append(sentence_text)\n",
        "        current_chunk_size += sentence_length\n",
        "\n",
        "        if current_chunk_size >= min_chunk_size:\n",
        "            chunks.append(\" \".join(current_chunk))\n",
        "            current_chunk = []\n",
        "            current_chunk_size = 0\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(\" \".join(current_chunk))\n",
        "\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "BEJVRxZ1xmc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def index_documents(documents):\n",
        "    # Create Pinecone index\n",
        "    if 'business-qa' not in pc.list_indexes().names():\n",
        "        pc.create_index(\n",
        "            name='business-qa',\n",
        "            dimension=384,\n",
        "            metric='cosine',\n",
        "            spec=ServerlessSpec(\n",
        "                cloud='aws',\n",
        "                region='us-east-1'\n",
        "            )\n",
        "        )\n",
        "\n",
        "    index = pc.Index('business-qa')\n",
        "\n",
        "    # Generate embeddings and index documents\n",
        "    for i, doc in enumerate(documents):\n",
        "        embedding = model.encode(doc).tolist()\n",
        "        index.upsert(vectors=[(str(i), embedding, {'text': doc})])"
      ],
      "metadata": {
        "id": "50UOpJDqFS_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function for searching the documents"
      ],
      "metadata": {
        "id": "uRGR7QhG2RpI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def search_documents(query, k=3):\n",
        "    index = pc.Index('business-qa')\n",
        "    query_embedding = model.encode(query).tolist()\n",
        "    results = index.query(vector=query_embedding, top_k=k, include_metadata=True)\n",
        "    return [result.metadata['text'] for result in results.matches]"
      ],
      "metadata": {
        "id": "jjQ5vgjLFWEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code for generating responses from the LLM. (Mixtral-8 7B in this case)"
      ],
      "metadata": {
        "id": "ep-5jBKU2WFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(question, context):\n",
        "    prompt = f\"\"\"\n",
        "    Context: {context}\n",
        "\n",
        "    Human: {question}\n",
        "\n",
        "    Assistant: Based on the context provided, I'll answer the human's question to the best of my ability.\n",
        "    \"\"\"\n",
        "\n",
        "    response = groq_client.chat.completions.create(\n",
        "        model=\"mixtral-8x7b-32768\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant for a business company called Yardstick.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        max_tokens=1024\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "XMCcRtDVFYq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def qa_bot(question):\n",
        "    # Retrieve relevant documents\n",
        "    relevant_docs = search_documents(question)\n",
        "\n",
        "    # Combine retrieved documents into a single context\n",
        "    context = \" \".join(relevant_docs)\n",
        "\n",
        "    # Generate response\n",
        "    answer = generate_response(question, context)\n",
        "\n",
        "    return answer"
      ],
      "metadata": {
        "id": "1rNCzf0MFcNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_text_chunks(file_path):\n",
        "    \"\"\"\n",
        "    Loads text from a file and returns a list of semantically chunked text.\n",
        "\n",
        "    Parameters:\n",
        "    file_path (str): The path to the text file.\n",
        "\n",
        "    Returns:\n",
        "    list: A list of strings, each representing a semantically coherent chunk of text from the file.\n",
        "    \"\"\"\n",
        "    with open(file_path, 'r') as file:\n",
        "        text = file.read()\n",
        "\n",
        "    return semantic_chunking(text)"
      ],
      "metadata": {
        "id": "VXX8-mqFP6GZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = 'Yardstick_text.txt'\n",
        "documents = load_text_chunks(file_path)\n",
        "index_documents(documents)"
      ],
      "metadata": {
        "id": "fcoY06WA6xHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The final chatbot"
      ],
      "metadata": {
        "id": "_YjZwrGF223u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    question = input(\"User: \")\n",
        "    if question.lower() == 'exit':\n",
        "        break\n",
        "    else:\n",
        "        answer = qa_bot(question)\n",
        "        print(f\"Bot: {answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxiiLdU33QW0",
        "outputId": "8578d1ae-9858-4332-b69d-48b54c32bb44"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User: hi\n",
            "Bot: Hello! How can I assist you today regarding Yardstick, Ayodhaya GPT, SEO, Live AI, Sales, or ChatBot? If you have any questions or ideas related to the blog post \"The Betrayal of a Vision: Elon Musk's Legal Battle with OpenAI and Sam Altman,\" I'm here to help. I can also provide information on the related posts, India's New AI Regulation or the competition in AI video generation with Haiper. Please feel free to ask anything!\n",
            "User: tell me what kind of business do you do?\n",
            "Bot: Yardstick is a business company that specializes in an interdisciplinary approach, leveraging cutting-edge technology to deliver high-value solutions across various use cases. A significant aspect of their offering is AI Integration, where they seamlessly integrate AI models into their clients' existing systems. This AI integration aims to enhance efficiency and drive business growth, ultimately adding value to their clients' operations.\n",
            "User: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8YWBJGok6t7g"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uyOiFaYa3hmH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}